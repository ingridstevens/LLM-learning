{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('/Users/ingrid/Downloads/compact-guide-to-large-language-models.pdf')\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the text splitter\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=200, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our splits from the PDF\n",
    "docs = r_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embeddings & Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can test different types of searches (similartiy search, mmr, etc.)\n",
    "question = \"What are the top in demand skills for data professionals?\"\n",
    "found_docs = qdrant.similarity_search(question)\n",
    "# found_docs = qdrant.max_marginal_relevance_search(query, k=2, fetch_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Use this code to use Ollama with llama2 or mistral models\n",
    "# from langchain.chat_models import ChatOllama\n",
    "# llm = ChatOllama(model_name=\"llama2\", temperature=0)\n",
    "\n",
    "##### Use this code to connect with OpenAI API\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1: RetreivalQA Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=qdrant.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The development of Large Language Models (LLMs) can be traced back to the 1950s when initial attempts were made to map hard rules around languages and follow logical steps to accomplish tasks like translation. However, these strictly defined rules only worked for concrete, well-defined tasks that the system had knowledge about.\\n\\nIn the 1990s, language models started evolving into statistical models, and language patterns began to be analyzed. However, larger-scale projects were limited by computing power.\\n\\nAdvancements in machine learning in the 2000s increased the complexity of language models, and the wide adoption of the internet provided a vast amount of training data.\\n\\nIn 2012, advancements in deep learning architectures and larger data sets led to the development of GPT (Generative Pre-trained Transformer). In 2018, Google introduced BERT (Bidirectional Encoder Representations from Transformers), which was a significant leap in architecture and paved the way for future large language models.\\n\\nOpenAI released GPT-3 in 2020, which became the largest model at the time with 175 billion parameters and set a new performance benchmark for language-related tasks.\\n\\nIn 2022, ChatGPT was launched, making GPT-3 and similar models widely accessible to users through a web interface, increasing public awareness of LLMs and generative AI.\\n\\nOverall, the development of LLMs has been driven by advancements in computing power, improvements in training data, and breakthroughs in deep learning architectures.'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"What is a breif historical background on the development of LLMs?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=qdrant.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What big breakthroughs happened with LLMs in 2023?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 2023, open source LLMs such as Dolly 2.0, LLaMA, Alpaca, and Vicuna showed increasingly impressive results. Additionally, GPT-4 was released, setting a new benchmark for both parameter size and performance. Thanks for asking!'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='a service that is widely accessible to users through a web interface  \\nand kicks off a huge increase in public awareness of LLMs and  \\ngenerative AI.\\n \\n2023   \\nOpen source LLMs begin showing increasingly impressive results  \\nwith releases such as Dolly 2.0, LLaMA, Alpaca and Vicuna.  \\nGPT-4 is also released, setting a new benchmark for both parameter  \\nsize and performance.', metadata={'source': '/Users/ingrid/Downloads/compact-guide-to-large-language-models.pdf', 'page': 2})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"source_documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ls__adf6707f28f14e0a84b131fd1c35d43d'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is no information provided in the given portion of the document about any specific breakthroughs or advancements related to LLMs in 2023.'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=qdrant.as_retriever(),\n",
    "    chain_type=\"map_reduce\"\n",
    ")\n",
    "result = qa_chain_mr({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In 2023, the field of Language Model Models (LLMs) witnessed several groundbreaking advancements that pushed the boundaries of language generation and understanding. These breakthroughs had a profound impact on the capabilities and accessibility of LLMs. Here are some notable developments:\\n\\n1. Enhanced Open Source LLMs: Open source LLMs like Dolly 2.0, LLaMA, Alpaca, and Vicuna underwent significant refinements. Researchers and developers dedicated their efforts to improving these models, resulting in enhanced language generation, comprehension, and overall effectiveness. These advancements contributed to more accurate and coherent language output.\\n\\n2. Introduction of GPT-4: The release of GPT-4 marked a major milestone in the LLM landscape. This highly anticipated model surpassed its predecessors in both parameter size and performance. With a larger model size, GPT-4 demonstrated superior language generation capabilities, producing more nuanced and contextually appropriate responses. Its enhanced architecture and training techniques further improved the quality of generated text.\\n\\n3. Increased Public Awareness: The breakthroughs in LLMs during 2023 garnered significant attention from the general public. Media coverage and discussions surrounding these advancements helped raise awareness about the potential of LLMs and their impact on various industries. This increased awareness led to a surge in interest and exploration of LLMs by individuals and organizations, fostering a broader understanding of their capabilities.\\n\\n4. User-Friendly Interfaces: Another notable development was the creation of user-friendly interfaces for LLMs. These interfaces made it easier for non-technical users to interact with LLMs, enabling a broader range of individuals to leverage the power of generative AI. The accessibility of LLMs through web interfaces facilitated their integration into various applications and services, making them more accessible and usable for a wider audience.\\n\\nOverall, the breakthroughs in LLMs in 2023 propelled the field forward, making significant strides in language generation, comprehension, and accessibility. These advancements not only showcased the potential of LLMs but also sparked widespread interest and adoption across different sectors.'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=qdrant.as_retriever(),\n",
    "    chain_type=\"refine\"\n",
    ")\n",
    "result = qa_chain_mr({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
